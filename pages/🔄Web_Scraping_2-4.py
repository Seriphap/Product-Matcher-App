import streamlit as st
import requests
from lxml import html
from urllib.parse import urljoin, quote_plus
import pandas as pd
from io import BytesIO
from PIL import Image
import xlsxwriter
from pymongo import MongoClient
from gridfs import GridFS
from rapidfuzz import fuzz
import time
import random


st.title("ğŸ” Scrape Alibaba Products and Export")

base_url = "https://fslidingfeng.en.alibaba.com/productgrouplist-822252468-"
headers = {'User-Agent': 'Mozilla/5.0'}

if "all_products" not in st.session_state:
Â Â Â  st.session_state.all_products = []

# Sidebar options
mode = st.sidebar.radio("ğŸ“„ Select scraping mode", ["Select page range", "Auto scrape all pages"])

if mode == "Select page range":
Â Â Â  FromPage = st.sidebar.text_input("From Page", value="1")
Â Â Â  ToPage = st.sidebar.text_input("To Page", value=f"{FromPage}")


# Scraping logic
if st.sidebar.button("ğŸš€ Start Scraping"):
Â Â Â  try:
Â Â Â Â Â Â Â  all_products = []

Â Â Â Â Â Â Â  if mode == "Select page range":
Â Â Â Â Â Â Â Â Â Â Â  for page in range(int(FromPage), int(ToPage) + 1):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  url = f"{base_url}{page}/Blow_Molding_Machines.html
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  response = requests.get(url, headers=headers)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  response.raise_for_status()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  time.sleep(random.uniform(2.5, 4.5))

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  tree = html.fromstring(response.content)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  product_xpath = '//*[@id="8919138061"]/div/div/div/div/div[2]/div'
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  product_elements = tree.xpath(product_xpath)

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  for product in product_elements:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  raw_html = html.tostring(product, encoding='unicode')
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  all_products.append({"raw_html": raw_html})

Â Â Â Â Â Â Â  else:Â  # Auto scrape all pages
Â Â Â Â Â Â Â Â Â Â Â  page = 1
Â Â Â Â Â Â Â Â Â Â Â  while True:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  url = f"{base_url}{page}/Blow_Molding_Machines.html
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  response = requests.get(url, headers=headers)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  response.raise_for_status()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  time.sleep(random.uniform(2.5, 4.5))

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  tree = html.fromstring(response.content)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  product_xpath = '//*[@id="8919138061"]/div/div/div/div/div[2]/div/div'
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  product_elements = tree.xpath(product_xpath)

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  if not product_elements:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  break

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  for product in product_elements:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  raw_html = html.tostring(product, encoding='unicode')
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  all_products.append({"raw_html": raw_html})

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  page += 1
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  time.sleep(random.uniform(2.5, 4.5))

Â Â Â Â Â Â Â  st.session_state.all_products = all_products
Â Â Â Â Â Â Â  st.success(f"âœ… Scraped {len(all_products)} products")

Â Â Â  except Exception as e:
Â Â Â Â Â Â Â  st.error(f"âŒ Error occurred: {e}")

# Function to extract name and image URL from raw HTML
def extract_name_and_image(raw_html):
Â Â Â  try:
Â Â Â Â Â Â Â  tree = html.fromstring(raw_html)
Â Â Â Â Â Â Â  name = ''.join(tree.xpath('.//div[1]//text()')).strip()
Â Â Â Â Â Â Â  image_url = tree.xpath('.//a/div/img/@src')
Â Â Â Â Â Â Â  image_url = image_url[0] if image_url else ""
Â Â Â Â Â Â Â  return name, image_url
Â Â Â  except:
Â Â Â Â Â Â Â  return "", ""

# Display and export
if st.session_state.all_products:
Â Â Â  st.markdown("### ğŸ” Search Product Name")
Â Â Â  search_query = st.text_input("Enter keyword to filter products")
Â Â Â  fuzzy_option = st.checkbox("ğŸ” Enable Fuzzy Search (similar words)", value=False)

Â Â Â  if search_query:
Â Â Â Â Â Â Â  if fuzzy_option:
Â Â Â Â Â Â Â Â Â Â Â  filtered_products = [
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  p for p in st.session_state.all_products
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  if fuzz.partial_ratio(search_query.lower(), extract_name_and_image(p["raw_html"])[0].lower()) > 70
Â Â Â Â Â Â Â Â Â Â Â  ]
Â Â Â Â Â Â Â  else:
Â Â Â Â Â Â Â Â Â Â Â  filtered_products = [
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  p for p in st.session_state.all_products
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  if search_query.lower() in extract_name_and_image(p["raw_html"])[0].lower()
Â Â Â Â Â Â Â Â Â Â Â  ]
Â Â Â  else:
Â Â Â Â Â Â Â  filtered_products = st.session_state.all_products

Â Â Â  st.markdown("### ğŸ–¼ï¸ Product Gallery")
Â Â Â  for i in range(0, len(filtered_products), 2):
Â Â Â Â Â Â Â  cols = st.columns(2)
Â Â Â Â Â Â Â  for j in range(2):
Â Â Â Â Â Â Â Â Â Â Â  if i + j < len(filtered_products):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  with cols[j]:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  st.markdown(filtered_products[i + j]["raw_html"], unsafe_allow_html=True)

Â Â Â  # Export to CSV
Â Â Â  if st.sidebar.button("ğŸ“¥ Download CSV"):
Â Â Â Â Â Â Â  export_data = []
Â Â Â Â Â Â Â  for item in filtered_products:
Â Â Â Â Â Â Â Â Â Â Â  name, image_url = extract_name_and_image(item["raw_html"])
Â Â Â Â Â Â Â Â Â Â Â  export_data.append({"name": name, "image_url": image_url})
Â Â Â Â Â Â Â  csv = pd.DataFrame(export_data).to_csv(index=False).encode('utf-8')
Â Â Â Â Â Â Â  st.sidebar.download_button("Save CSV File", data=csv, file_name="products.csv", mime="text/csv")

Â Â Â  # Export to Excel
Â Â Â  if st.sidebar.button("ğŸ“¥ Download Excel"):
Â Â Â Â Â Â Â  output = BytesIO()
Â Â Â Â Â Â Â  workbook = xlsxwriter.Workbook(output, {'in_memory': True})
Â Â Â Â Â Â Â  worksheet = workbook.add_worksheet("Products")

Â Â Â Â Â Â Â  worksheet.write("A1", "Product Name")
Â Â Â Â Â Â Â  worksheet.write("B1", "Image URL")
Â Â Â Â Â Â Â  worksheet.write("C1", "Image")

Â Â Â Â Â Â Â  row = 1
Â Â Â Â Â Â Â  for item in filtered_products:
Â Â Â Â Â Â Â Â Â Â Â  name, image_url = extract_name_and_image(item["raw_html"])
Â Â Â Â Â Â Â Â Â Â Â  worksheet.write(row, 0, name)
Â Â Â Â Â Â Â Â Â Â Â  worksheet.write(row, 1, image_url)

Â Â Â Â Â Â Â Â Â Â Â  try:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  img_response = requests.get(image_url, stream=True, timeout=10)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  if img_response.status_code == 200:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  img = Image.open(BytesIO(img_response.content))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  img.thumbnail((100, 100))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  img_byte_arr = BytesIO()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  img.save(img_byte_arr, format='PNG')
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  worksheet.insert_image(row, 2, name + ".png", {
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  'image_data': img_byte_arr,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  'x_scale': 1,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  'y_scale': 1
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  })
Â Â Â Â Â Â Â Â Â Â Â  except:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  pass
Â Â Â Â Â Â Â Â Â Â Â  row += 1

Â Â Â Â Â Â Â  workbook.close()
Â Â Â Â Â Â Â  output.seek(0)
Â Â Â Â Â Â Â  st.sidebar.download_button(
Â Â Â Â Â Â Â Â Â Â Â  label="Save Excel File",
Â Â Â Â Â Â Â Â Â Â Â  data=output,
Â Â Â Â Â Â Â Â Â Â Â  file_name="products_with_images.xlsx",
Â Â Â Â Â Â Â Â Â Â Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
Â Â Â Â Â Â Â  )

# MongoDB Upload
st.sidebar.markdown("### â˜ï¸ MongoDB Atlas Upload")
username = st.sidebar.text_input("Username")
password = st.sidebar.text_input("Password", type="password")

if username and password and st.sidebar.button("â˜ï¸ Upload to MongoDB Atlas"):
Â Â Â  try:
Â Â Â Â Â Â Â  encoded_password = quote_plus(password)
Â Â Â Â Â Â Â  mongo_uri = f"mongodb+srv://{username}:{encoded_password}@cluster0.hnvlg44.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
Â Â Â Â Â Â Â  client = MongoClient(mongo_uri)
Â Â Â Â Â Â Â  db = client["productDB"]
Â Â Â Â Â Â Â  collection = db["products"]
Â Â Â Â Â Â Â  fs = GridFS(db)

Â Â Â Â Â Â Â  # Clear old data
Â Â Â Â Â Â Â  collection.delete_many({})
Â Â Â Â Â Â Â  db.fs.files.delete_many({})
Â Â Â Â Â Â Â  db.fs.chunks.delete_many({})

Â Â Â Â Â Â Â  for item in st.session_state.all_products:
Â Â Â Â Â Â Â Â Â Â Â  name, image_url = extract_name_and_image(item["raw_html"])
Â Â Â Â Â Â Â Â Â Â Â  img_response = requests.get(image_url, stream=True, timeout=10)
Â Â Â Â Â Â Â Â Â Â Â  if img_response.status_code == 200:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  img_bytes = BytesIO(img_response.content)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  image_id = fs.put(img_bytes, filename=name + ".png")

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  collection.insert_one({
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  "name": name,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  "image_url": image_url,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  "image_file_id": image_id
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  })

Â Â Â Â Â Â Â  st.sidebar.success("âœ… Uploaded products and images to MongoDB Atlas")

Â Â Â  except Exception as e:
Â Â Â Â Â Â Â  st.sidebar.error(f"âŒ Upload failed: {e}")


