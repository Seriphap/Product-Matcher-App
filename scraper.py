import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import uuid
from urllib.parse import urljoin

st.title("üõ†Ô∏è Product Scraper (Column-Row Structure)")
st.write("‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (‡∏ä‡∏∑‡πà‡∏≠ + ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û) ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå hsc-spareparts.com ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á column/row")

if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"):
    headers = {'User-Agent': 'Mozilla/5.0'}
    all_products = []
    os.makedirs("images", exist_ok=True)

    progress = st.progress(0)
    total_pages = 39

    for page in range(1, total_pages + 1):
        url = f'https://hsc-spareparts.com/products/{page}.html'
        st.write(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤ {page}...")

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
        except Exception as e:
            st.warning(f"‚ùå ‡∏´‡∏ô‡πâ‡∏≤ {page} ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {e}")
            continue

        soup = BeautifulSoup(response.text, 'html.parser')
        plist = soup.find('div', id='plist')
        if not plist:
            st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö div#plist ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}")
            continue
        product_container = plist.find_all('div')[2] if len(plist.find_all('div')) >= 3 else None
        if not product_container:
            st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö container ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page}")
            continue

        for col in product_container.find_all('div', recursive=False):
            for row in col.find_all('div', recursive=False):
                a_tag = row.find('a')
                img_tag = a_tag.find('img') if a_tag else None
                name_tag = a_tag.find('p') if a_tag else None

                image_url = img_tag['src'] if img_tag and 'src' in img_tag.attrs else None
                full_image_url = urljoin(url, image_url) if image_url else 'N/A'
                product_name = name_tag.text.strip() if name_tag else 'N/A'
                image_filename = f"{uuid.uuid4().hex}_{os.path.basename(full_image_url)}" if image_url else 'N/A'

                # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                if image_url:
                    try:
                        img_data = requests.get(full_image_url, headers=headers).content
                        with open(os.path.join("images", image_filename), 'wb') as f:
                            f.write(img_data)
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {full_image_url} - {e}")
                        image_filename = 'N/A'

                all_products.append({
                    'Product Name': product_name,
                    'Image URL': full_image_url,
                    'Image File': image_filename
                })

        progress.progress(page / total_pages)

    df = pd.DataFrame(all_products)
    st.success("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
    st.dataframe(df)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    st.subheader("üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤")
    for product in all_products[:5]:
        image_path = os.path.join("images", product['Image File'])
        if os.path.exists(image_path):
            st.image(image_path, caption=product['Product Name'])

    # ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button("üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î CSV", csv, "products.csv", "text/csv")

